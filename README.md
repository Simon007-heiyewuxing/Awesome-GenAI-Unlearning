# Awesome-GenAI-Unlearning
![PRs Welcome](https://img.shields.io/badge/PRs-Welcome-green)  [![Awesome](https://awesome.re/badge.svg)](https://awesome.re) 


This repository contains a list of papers on Generative AI Machine Unlearning based on our survey paper: [**Machine Unlearning in Generative AI: A Survey**](https://arxiv.org/abs/2312.11518) (*[Zheyuan (Frank) Liu](https://franciscoliu.github.io/), [Guangyao Dou](https://guangyaodou.github.io/), [Zhaoxuan Tan](https://zhaoxuan.info/), [Yijun Tian](https://www.yijuntian.com/) and [Meng Jiang](http://www.meng-jiang.com/)*).
We categorize existing works based on their [approaches](), modality and [applications](). Additionally, we include datasets and benchmarks for various unlearning scenarios. 


## Table of Contents
- [Awesome-GenAI-Unlearning ](#awesome-genai-unlearning-)
  - [Table of Contents](#table-of-contents)
  - [Datasets, Benchmarks](#datasets-benchmarks)
  - [Generative Image Models](#generative-image-models)
  - [Large Language Models (LLMs)](#large-language-models-(LLMs))
  - [Large Multimodal Models (LMMs)](#large-multimodal-models-(LMMs))

  
## Datasets, Benchmarks:

### Datasets:
#### Safety Alignment
- **LAION**
- **Civil Comments**
- **PKU-SafeRLHF**
- **Anthropic red team**

#### Copyrights Protection
- **Harry Potter**
- **Bookcorpus**
- **TOFU**

#### Hallucination Reduction 
- **HaluEVAL**
- **TruthfulQA**
- **CounterFact**
- **ZsRE**
- **MSCOCO**

#### Privacy Compliance
- **Pile**
- **Yelp/Amazon Reviews**
- **SST-2**
- **PersonaChat**
- **LEDGAR**
- **SAMsum**
- **IMDB**
- **CeleA-HQ**
- **I2P**

#### Bias/Unfairness Alleviation
- **StereoSet**
- **HateXplain**
- **CrowS Pairs**
- **Anthropic red team**

### Benchmarks:
#### Generative Image Models
- **UnlearnCanvas**

#### LLMs
- **TOFU**
- **WMDP**


#### LMMs
- **Object HalBench:** [EMNLP 2018] Object Hallucination in Image Captioning ([code](https://github.com/LisaAnne/Hallucination))
- **MHumanEval:** [CVPR'24] RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback ([code](https://github.com/RLHF-V/RLHF-V))
- **LLaVA Bench:** [Neurips 2023 (oral)] Visual Instruction Tuning ([code](https://github.com/haotian-liu/LLaVA/blob/main/docs/LLaVA_Bench.md))
- **MMHal-Bench:** Aligning Large Multimodal Models with Factually Augmented RLHF ([code](https://huggingface.co/datasets/Shengcao1006/MMHal-Bench))
- **POPE:** [EMNLP 2023] POPE: Polling-based Object Probing Evaluation for Object Hallucination ([code](https://github.com/RUCAIBox/POPE))

## Generative Image Models:


## Large Language Models (LLMs):


## Large Multimodal Models (LMMs):


## Applications:
### Safety Alignment:


### Copyright Protection:


### Hallucination Reduction:

### Privacy Compliance:

### Bias/Unfairness Alleviation:






## Contributing:
üëç Contributions to this repository are welcome! 
We will try to make this list updated. If you find any error or any missed paper, please don't hesitate to open an issue or pull request.
